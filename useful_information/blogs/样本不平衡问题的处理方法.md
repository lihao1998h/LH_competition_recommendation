# 一、样本不平衡问题

样本不平衡问题主要存在于有监督机器学习任务中。例如，当训练集中的样本有90%的数据标签都为0，而只有10%为1，那么我只要让学习器遇到任何样本都返回0则可以达到90%的准确率，那何来泛化？
# 二、处理方法
## 2.1 重采样
既然原样本不平衡，那么最容易想到的方法就是重新获得样本。

### 2.1.1. 欠采样

欠采样是通过**减少**丰富类的大小来平衡数据集，当数据量足够时就该使用此方法。通过保存所有稀有类样本，并在丰富类别中随机选择与稀有类别样本相等数量的样本，可以检索平衡的新数据集以进一步建模。

欠采样的缺点显而易见，那就是最终的训练集丢失了数据，模型只学到了总体模式的一部分。

因为欠采样会丢失信息，如何减少信息的损失呢？
1. 第一种方法叫做EasyEnsemble，利用模型融合的方法（Ensemble）：多次欠采样（放回采样，这样产生的训练集才相互独立）产生多个不同的训练集，进而训练多个不同的分类器，通过组合多个分类器的结果得到最终的结果。
2. 第二种方法叫做BalanceCascade，利用增量训练的思想（Boosting）：先通过一次欠采样产生训练集，训练一个分类器，对于那些分类正确的大众样本不放回，然后对这个更小的大众样本欠采样产生训练集，训练第二个分类器，以此类推，最终组合所有分类器的结果得到最终结果。
3. 第三种方法是利用KNN试图挑选那些最具代表性的大众样本，叫做NearMiss，这类方法计算量很大。

还有一种方法就是，假设少数类样本数量为N，那就将多数类样本分为N个簇，取每个簇的中心点作为多数类的新样本，再加上少数类的所有样本进行训练。这样就可以保证了多数类样本在特征空间的分布特性。
### 2.1.2. 过采样
相反，当数据量不足时就应该使用过采样，它尝试通过**增加**稀有样本的数量来平衡数据集，而不是去除丰富类别的样本的数量。





过采样会把小众样本复制多份，一个点会在高维空间中反复出现，这会导致一个问题，那就是运气好就能分对很多点，否则分错很多点。为了解决这一问题，可以在每次生成新数据点时加入轻微的随机扰动，经验表明这种做法非常有效。

常用的方法有：SMOTE、重复、自举、合成少数类。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20191109091708238.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjI5Nzg1NQ==,size_16,color_FFFFFF,t_70)
SMOTE为每个小众样本合成相同数量的新样本，这带来一些潜在的问题：一方面是增加了类之间重叠的可能性，另一方面是生成一些没有提供有益信息的样本。为了解决这个问题，出现两种方法：Borderline-SMOTE与ADASYN。

　　Borderline-SMOTE的解决思路是寻找那些应该为之合成新样本的小众样本。即为每个小众样本计算K近邻，只为那些K近邻中有一半以上大众样本的小众样本生成新样本。直观地讲，只为那些周围大部分是大众样本的小众样本生成新样本，因为这些样本往往是边界样本。确定了为哪些小众样本生成新样本后再利用SMOTE生成新样本。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20191109091842145.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjI5Nzg1NQ==,size_16,color_FFFFFF,t_70)

>欠采样和过采样这两种方法相比而言，都没有绝对的优势。这两种方法的应用取决于它适用的用例和数据集本身。另外将过采样和欠采样结合起来使用也是成功的。
## 2.2 使用K-fold交叉验证
值得注意的是，使用过采样方法来解决不平衡问题时应适当地应用交叉验证。这是因为过采样会观察到罕见的样本，并根据分布函数应用自举生成新的随机数据，如果在过采样之后应用交叉验证，那么我们所做的就是将我们的模型过拟合于一个特定的人工引导结果。这就是为什么在过度采样数据之前应该始终进行交叉验证，就像实现特征选择一样。只有重复采样数据可以将随机性引入到数据集中，以确保不会出现过拟合问题。

K-fold交叉验证就是把原始数据随机分成K个部分，在这K个部分中选择一个作为测试数据，剩余的K-1个作为训练数据。交叉验证的过程实际上是将实验重复做K次，每次实验都从K个部分中选择一个不同的部分作为测试数据，剩余的数据作为训练数据进行实验，最后把得到的K个实验结果平均。

## 2.3 转化为一分类问题
对于二分类问题，如果正负样本分布比例极不平衡，我们可以换一个完全不同的角度来看待问题：把它看做一分类（One Class Learning）或异常检测（Novelty Detection）问题。这类方法的重点不在于捕捉类间的差别，而是为其中一类进行建模，经典的工作包括One-class SVM等，如下图所示：



One Class SVM 是指你的训练数据只有一类正（或者负）样本的数据， 而没有另外的一类。在这时，你需要学习的实际上你训练数据的边界。而这时不能使用最大化软边缘了，因为你没有两类的数据。 所以呢，在这边文章中，“Estimating the support of a high-dimensional distribution”， Sch?lkopf 假设最好的边缘要远离特征空间中的原点。左边是在原始空间中的边界，可以看到有很多的边界都符合要求，但是比较靠谱的是找一个比较紧的边界（红色的）。这个目标转换到特征空间就是找一个离原点比较远的边界，同样是红色的直线。当然这些约束条件都是人为加上去的，你可以按照你自己的需要采取相应的约束条件。比如让你data 的中心离原点最远。

说明：对于正负样本极不均匀的问题，使用异常检测，或者一分类问题，也是一个思路。

## 2.4 组合不同的重采样数据集
成功泛化模型的最简单方法是使用更多的数据，问题是像逻辑回归或随机森林这样开箱即用的分类器，倾向于通过舍去稀有类来泛化模型。一个简单的最佳实践是建立n个模型，每个模型使用稀有类别的所有样本和丰富类别的n个不同样本。假设想要合并10个模型，那么将保留例如1000例稀有类别，并随机抽取10000例丰富类别。然后，只需将10000个案例分成10块，并训练10个不同的模型。



如果拥有大量数据，这种方法是简单并且是可横向扩展的，这是因为可以在不同的集群节点上训练和运行模型。集合模型也趋于泛化，这使得该方法易于处理。

## 2. 5 用不同比例重新采样
可以很好地将稀有类别和丰富类别之间的比例进行微调，最好的比例在很大程度上取决于所使用的数据和模型。但是，不是在整体中以相同的比例训练所有模型，所以值得尝试合并不同的比例。如果10个模型被训练，有一个模型比例为1：1（稀有：丰富）和另一个1：3甚至是2：1的模型都是有意义的。一个类别获得的权重依赖于使用的模型。



## 2.6 多模型Bagging
方法5 虽然能够选出最好的样本数据比例。但是它的鲁棒性不能够保证：它的鲁棒性取决于测试集样本的选取。

为了解决上述方法的缺陷，增加模型鲁棒性。为此，我本人在 随机森林算法 思想的启发下，想出了在上述方法的基础上，将不同比例下训练出来的模型进行 多模型Bagging 操作，具体的步骤如下：

1. 对两类样本选取 N 组不同比例的数据进行训练并测试，得出模型预测的准确率： 

P={ Pi | i=1,2,...N }

2. 对上述各模型的准确率进行归一化处理，得到新的权重分布：

Ω={ ωi | i=1,2,...N }

其中：



3. 按权重分布 Ω 组合多个模型，作为最终的训练器：

● 对于分类任务：



● 对于回归任务：



7、集群丰富类
Sergey Quora提出了一种优雅的方法，他建议不要依赖随机样本来覆盖训练样本的种类，而是将r个群体中丰富类别进行聚类，其中r为r中的例数。每个组只保留集群中心（medoid）。然后，基于稀有类和仅保留的类别对该模型进行训练。

7.1. 对丰富类进行聚类操作

首先，我们可以对具有大量样本的丰富类进行聚类操作。假设我们使用的方法是 K-Means聚类算法 。此时，我们可以选择K值为稀有类中的数据样本的个数，并将聚类后的中心点以及相应的聚类中心当做富类样本的代表样例，类标与富类类标一致。



7.2. 聚类后的样本进行有监督学习

经过上述步骤的聚类操作，我们对富类训练样本进行了筛选，接下来我们就可以将相等样本数的K个正负样本进行有监督训练。如下图所示：



8、设计适用于不平衡数据集的模型
所有之前的方法都集中在数据上，并将模型保持为固定的组件。但事实上，如果设计的模型适用于不平衡数据，则不需要重新采样数据，著名的XGBoost已经是一个很好的起点，因此设计一个适用于不平衡数据集的模型也是很有意义的。

通过设计一个代价函数来惩罚稀有类别的错误分类而不是分类丰富类别，可以设计出许多自然泛化为稀有类别的模型。例如，调整SVM以惩罚稀有类别的错误分类。





参考资料：
1. [怎样解决样本不平衡问题？](https://www.cnblogs.com/guoruibing/articles/9561035.html)
2. [如何解决样本不均衡的问题](https://www.jianshu.com/p/76dce1fca85b)
